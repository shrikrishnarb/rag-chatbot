[
  {
    "page": 1,
    "text": "7\n \n \nCHAPTER 2  \n \nLITERATURE SURVEY \n \n2.1 A BRIEF HISTORY OF QUANTUM COMPUTING: \n \n         The idea of computational device based on quantum mechanics was first explored \nin the 1970’s and early 1980’s by physicists and computer scientists such as Charles H. \nBennet of the IBM Thomas J. Watson Research Centre, Paul A. Beniof of Arogonne \nNational Laboratory in Illinois, David Deustch of the University of Oxford and Richard \nP. Feynman of Caltech. The idea emerged when scientists were pondering on the \nfundamental limits of computation. In 1982 Feynman was among the fewer to attempt to \nprovide conceptually a new kind of computers which could be devised based on the \nprinciples of quantum physics. He constructed an abstract model to show how a quantum \nsystem could be used to do computations and also explain how such a machine would be \nable to act as a simulator for physical problems pertaining to quantum physics. In other \nwords, a physicist would have the ability to carry out experiments in quantum physics \ninside a quantum mechanical computer. Feynman further analysed that quantum \ncomputers can solve quantum mechanical many body problems that are impractical to \nsolve on a classical computer. This is due to the fact that solutions on a classical \ncomputer would require exponentially growing time where as the whole calculations on \nquantum computer can be done in polynomial time. \n          \n        Later, in 1985, Deutsch realized that Feynman assertion could eventually lead to a \ngeneral-purpose quantum computer. He showed that any physical process, in principle \ncould be modelled perfectly by a quantum computer. Thus, a quantum computer would \nhave capabilities far beyond those of any traditional classical computer. Consequently \nefforts were made to find interesting applications for such a machine. This did not lead to \nmuch success except continuing few mathematical problems. Peter Shor in 1994 set out a \nmethod for using quantum computers to crack an important problem in number theory \nwhich was namely factorisation. He showed how an ensemble of mathematical \noperations, designed specifically for a quantum computer could be organized to make \nsuch a machine to factor huge numbers extremely rapidly, much faster than is possible on \nconventional computers. With this breakthrough, quantum computing transformed from a \nmere academic curiosity directly to an interest world over. \n          \n        Perhaps the most astonishing fact about quantum computing is that it took \nexceedingly large time to take off. Physicists have known since 1920’s that the world of \nsubatomic particles is a realm apart, but it took computer scientists another half century \nto begin wondering whether quantum effects might be harnessed for computation. The \nanswer was far from obvious."
  },
  {
    "page": 2,
    "text": "8\n2.2 LIMITATIONS OF CLASSICAL COMPUTER AND BIRTH OF ART OF QUANTUM \nCOMPUTING \n \n2.2.1:  Public Key Cryptography and Classical factoring of big integers: \n \n          In 1970 a clever mathematical discovery in the shape of “public key” systems \nprovided a solution to key distribution problem. In these systems users do not need to \nagree on a secret key before they send the message. The principle of a safe with two keys, \none public key to lock it, and another private one to open it, is employed. Everyone has a \nkey to lock the safe but one person has a key that will open it again, so anyone can put a \nmessage in the safe but only one person can take it out. In practice the two keys are two \nlarge integer numbers. One can easily derive a public key from a private key but not vice \nversa. The system exploits the fact that certain mathematical operations are easier to \nperform in one direction that the other e.g. multiplication of numbers can be performed \nmuch faster than factorising a large number. What really counts for a “fast” algorithm is \nnot the actual time taken to multiply a particular pairs of numbers but the fact that the \ntime does not increase too sharply when we apply the same method to ever-large \nnumbers. We know that multiplication requires little extra time when we switch from two \nthree digit numbers to two thirty digit numbers using the simpler trial division method \nabout 10∧13 times more time or memory consuming than factoring a three digit number. \nIn case of factorisation the use of computational resources is enormous when we keep on \nincreasing the number of digits. As a matter of fact public key cryptosystems could avoid \nkey distribution problem. However their security depends upon unproven mathematical \nassumptions such as the difficulty of factoring large integers. Nevertheless one such \nprotocol is RSA, which maps electronic banking possible by assuming banks and their \ncustomers that a bogus transfer of funds or a successful forgery would take the world’s \nfastest computer millions of years to carry out. Another is the under spread Data \nEncryption Standard (DES) which remains secure far most ordinary business \ntransactions.  \n \n          The procedure of factorising a large integer can be quantified as follows. Consider \na number N with L decimal digits (N ~ 10 to power L). The number is factored using trial \ndivision method. On conventional computers one of well known factoring algorithm runs \nfor number of operations of the order of  \n \ns ~ O (exp ( (64/9)1/3 (lnN)1/3 (ln lnN)2/3 )) \nor explicitly,   \ns ~ A exp ( 1.9 L1/3 ( lnL)2/3 ) \n \n \nThis algorithm therefore, scales exponentially with the input size log N (log N determines \nthe length of the input. The base of the logarithm is determined by our numbering system. \nThus base 2 gives the length in binary, a base 2 gives the length in binary, a base of 10 in \ndecimal and so on) e.g. in 1994 a 129 digit number (known as RSA 129) was successfully \nfactored using this algorithm on approximately 1600 workstations scattered around the \nworld, the entire factorisation took eight months. Using this to estimate the per factor of \nthe above exponential scaling, it is found that it would take roughly 800,000 years to \nfactor a 250 digit number with the same computer power, similarly a 1000 digit number \nwould require 10 to the power 25 years (much longer than the age of universe). The \ndifficulty of factoring large numbers is crucial for public key cryptography such as used"
  }
]